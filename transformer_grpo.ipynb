{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import profiler as profiler\n",
    "from tokenizer import NaiveTokenizer\n",
    "from transformer import DecoderTrans \n",
    "from dataclasses import dataclass\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.set_printoptions(linewidth=10000)\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the input dataset\n",
    "with open('input.txt') as f:\n",
    "    data = f.read()\n",
    "\n",
    "naive_tokenizer = NaiveTokenizer(data)\n",
    "vocab_size = naive_tokenizer.vocab_size\n",
    "# Generate the encoded dataset\n",
    "dataset = naive_tokenizer.encode(data)\n",
    "\n",
    "# Build the train/test datasets\n",
    "train_data_sz = int(len(dataset)*0.8)\n",
    "train_data = dataset[:train_data_sz]\n",
    "test_data = dataset[train_data_sz:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is  cuda\n"
     ]
    }
   ],
   "source": [
    "## Meta parameters \n",
    "seed = 2580 # Fixed random seed\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#Optimizer parameters\n",
    "learning_rate = 3e-4 # Learning rate for the optimizer\n",
    "nb_iter = 1000 # Number of iterations for the optimizer\n",
    "batch_size = 64 # Number of blocks in a batch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device is ', device)\n",
    "\n",
    "#Transformer  parameters\n",
    "@dataclass\n",
    "class TransformerParams:\n",
    "    vocab_size: int\n",
    "    n_embedding: int = 384 # Embedding size\n",
    "    n_decoder_blocks: int = 3 # Number of decoder blocks in the transformer\n",
    "    n_mha: int = 1 # Number of multi-head attention layers in each decoder block\n",
    "    n_heads: int = 2 # Number of heads per multi-head attention layer\n",
    "    individual_head_size: int = n_embedding // n_heads # Individual head size\n",
    "    block_size: int = 64 # Number of tokens in a block (aka context)\n",
    "    \n",
    "transformer_params = TransformerParams(vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get a batch of the split\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    ix = np.random.randint(0, len(data)-transformer_params.block_size, (batch_size,))\n",
    "\n",
    "    batch_x = torch.stack([torch.tensor(data[i:i+transformer_params.block_size]) for i in ix], dim=0).to(device)\n",
    "    batch_y = torch.stack([torch.tensor(data[i+1:i+transformer_params.block_size+1]) for i in ix], dim=0).to(device)\n",
    "\n",
    "    return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = DecoderTrans(transformer_params)\n",
    "transformer = transformer.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training loop for the base transformer\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=learning_rate) \n",
    "transformer.train()\n",
    "\n",
    "nb_iter = 5000\n",
    "for i_iter in range(nb_iter):\n",
    "    x, y = get_batch('train')\n",
    "    \n",
    "    y_pred, loss  = transformer('forward', x = x, y = y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Handle the case where the loss is not a scalar (due to DataParallel, e.g.)\n",
    "    if loss.ndim > 0:\n",
    "        loss = loss.mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss\n",
    "    if i_iter % 10 == 0:\n",
    "        print(f\"Iter {i_iter}, Loss {loss.item()}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rule based rewards for the generated strings\n",
    "def reward_short_sentences(tested_string):\n",
    "    split = str.split(tested_string, '\\n')\n",
    "    return (sum([len(s) > 10 and len(s) < 20 for s in split]))\n",
    "\n",
    "def reward_shouting(tested_string):\n",
    "    return sum([c.isupper() for c in tested_string])\n",
    "\n",
    "# Full reward function, let us only use the shouting reward for now\n",
    "def full_reward(tested_string):\n",
    "    return reward_shouting(tested_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the \"Old policy\" $\\pi_{\\theta_{old}}(o_t\\vert q, o_{<t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advantage computation : Process supervision and Outcome supervision as per DeepseekMath paper\n",
    "# In practice, we will use OS (DeepseekR1 paper GRPO loss is does not include the inner sum related to process supervision ?)\n",
    "\n",
    "def compute_process_supervision_advantage(generated_tokens, base_context, reward_fn):\n",
    "    context_size = base_context.shape[1]\n",
    "    nb_generated_tokens = generated_tokens.shape[1] - context_size\n",
    "    #Compute the rewards for every shots and every increments of tokens\n",
    "    R = []\n",
    "    for i_substring in range(nb_generated_tokens):\n",
    "        subtokens = generated_tokens[:,:i_substring+context_size+1]\n",
    "        substrings = [naive_tokenizer.decode(s.tolist()) for s in subtokens]\n",
    "        rewards = [reward_fn(s) for s in substrings]\n",
    "        R.append(rewards)\n",
    "    R = np.transpose(R) # (B,T)\n",
    "\n",
    "    #Collect statistics on the full flattened reward array (See DeepseekMath's paper)\n",
    "    mean_rewards = np.mean(R) \n",
    "    std_rewards = np.std(R)\n",
    "\n",
    "    normalized_rewards = (torch.from_numpy(R) - mean_rewards)/(std_rewards+1e-4)\n",
    "    \n",
    "    #Advantage is the cumulative sum of normalized rewards for i>t\n",
    "    advantages = torch.cumsum(normalized_rewards.flip(dims=[1]), dim=1).flip(dims=[1])\n",
    "    return advantages\n",
    "\n",
    "def compute_outcome_supervision_advantage(generated_tokens, base_context, reward_fn):\n",
    "    #Compute all rewards\n",
    "    rewards = np.array([reward_fn(naive_tokenizer.decode(s.tolist())) for s in generated_tokens])\n",
    "    \n",
    "    #Normalize the group rewards\n",
    "    mean_rewards = np.mean(rewards)\n",
    "    std_rewards = np.std(rewards)\n",
    "    normalized_rewards = (torch.from_numpy(rewards) - mean_rewards)/(std_rewards+1e-4)\n",
    "    \n",
    "    #Advantage is directly the normalized_rewards as per DeepseekMath paper\n",
    "    advantages = normalized_rewards\n",
    "    return advantages\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_grpo(generated_tokens, old_policy_logprobs, ref_policy_logprobs, advantages, model, query):\n",
    "    eps = 0.2\n",
    "    beta = 0.04\n",
    "    \n",
    "    #Detach the ref/old policy logprobs to avoid backpropagating through them\n",
    "    ref_policy_logprobs = ref_policy_logprobs.detach()\n",
    "    old_policy_logprobs = old_policy_logprobs.detach()\n",
    "    \n",
    "    #Compute the new policy logprobs\n",
    "    new_policy_logprobs = None\n",
    "    for i_substring in range(generated_tokens.shape[1]-1):\n",
    "        substring = generated_tokens[:,:i_substring+1]\n",
    "        token_eval = generated_tokens[:,i_substring+1].unsqueeze(1)\n",
    "        \n",
    "        token_logprob =  model('get_token_logprob', context=substring, token=token_eval)\n",
    "        \n",
    "        if new_policy_logprobs is None:\n",
    "            new_policy_logprobs = token_logprob\n",
    "        else:\n",
    "            new_policy_logprobs = torch.cat([new_policy_logprobs, token_logprob], dim=1)\n",
    "        \n",
    "    \n",
    "    pi_ref_log = torch.sum(ref_policy_logprobs, dim=1) \n",
    "    pi_old_log = torch.sum(old_policy_logprobs, dim=1)\n",
    "    pi_new_log = torch.sum(new_policy_logprobs, dim=1)\n",
    "    \n",
    "    policy_ratio = torch.exp(pi_new_log - pi_old_log)\n",
    "    clipped_policy_ratio = torch.clamp(policy_ratio, 1-eps, 1+eps)\n",
    "    \n",
    "    weighted_policy_ratio = torch.einsum('b,b->b', policy_ratio, advantages)\n",
    "    weighted_clipped_policy_ratio = torch.einsum('b,b->b', clipped_policy_ratio, advantages)\n",
    "    \n",
    "    def KL_divergence(pi_new_log, pi_ref_log):\n",
    "        return torch.exp(pi_ref_log - pi_new_log) - (pi_ref_log-pi_new_log) - 1\n",
    "    \n",
    "    policy_loss = torch.min(weighted_policy_ratio, weighted_clipped_policy_ratio)\n",
    "    kl_loss = KL_divergence(pi_new_log, pi_ref_log)\n",
    "    \n",
    "    loss = policy_loss - beta * kl_loss\n",
    "    \n",
    "    return torch.mean(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grpo_optim(model):\n",
    "    # Make an adamW optimizer\n",
    "    grpo_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, maximize=True) \n",
    "\n",
    "    # Set the model in training mode and disable dropout\n",
    "    model.train()\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model.module.disable_dropout()\n",
    "    else:\n",
    "        model.disable_dropout()\n",
    "        \n",
    "\n",
    "    nb_outer_grpo_updates = 100\n",
    "    nb_inner_grpo_updates = 10\n",
    "    grpo_n_shots = 10\n",
    "    grpo_n_tokens = 128\n",
    "\n",
    "    base_context = torch.tensor([naive_tokenizer.encode('\\n') for _ in np.arange(grpo_n_shots)], dtype=torch.long).to(device)\n",
    "\n",
    "    for i_outer in range(nb_outer_grpo_updates):\n",
    "        # Given query q, generate a batch of outputs o\n",
    "        \n",
    "        generated_tokens, generated_logprobs =  model('generate', context=base_context, nb_tokens=grpo_n_tokens )\n",
    "        \n",
    "        old_policy_logprobs = generated_logprobs\n",
    "        ref_policy_logprobs = generated_logprobs\n",
    "        # Precompute the advantages\n",
    "        advantages = compute_outcome_supervision_advantage(generated_tokens, base_context, full_reward).cuda()\n",
    "        advantages = advantages.to(device)\n",
    "\n",
    "        for i_inner in range(nb_inner_grpo_updates):\n",
    "            # Compute the loss\n",
    "            loss = loss_grpo(generated_tokens, old_policy_logprobs, ref_policy_logprobs, advantages, model, base_context)\n",
    "            # Backward pass\n",
    "            grpo_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            grpo_optimizer.step()\n",
    "            \n",
    "            \n",
    "            # Print the loss\n",
    "            print(f\"Iter  {i_outer}-{i_inner}, Loss {loss.item()}\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the reward pre grpo optimization\n",
    "with torch.no_grad():\n",
    "    reward = []   \n",
    "    for i in range(10):\n",
    "        print(f'Generation {i}')\n",
    "        base_context = torch.tensor([naive_tokenizer.encode('\\n') for _ in range(1)], dtype=torch.long).to(device)\n",
    "        generated_tokens, generated_logprobs =  transformer('generate',context=base_context, nb_tokens=1024)\n",
    "        reward.append([full_reward(naive_tokenizer.decode(g.tolist())) for g in generated_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "grpo_trans = DecoderTrans(transformer_params).to(device)\n",
    "grpo_trans.load_state_dict(copy.deepcopy(transformer.state_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_optim(grpo_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the reward pre grpo optimization\n",
    "with torch.no_grad():\n",
    "    post_reward = []   \n",
    "    for i in range(10):\n",
    "        print(f'Generation {i}')\n",
    "        base_context = torch.tensor([naive_tokenizer.encode('\\n') for _ in range(10)], dtype=torch.long).to(device)\n",
    "        generated_tokens, generated_logprobs =  grpo_trans('generate',context=base_context, nb_tokens=1024)\n",
    "        post_reward.append([full_reward(naive_tokenizer.decode(g.tolist())) for g in generated_tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Average reward for the base transformer : {np.mean(np.array(reward))}')\n",
    "print(f'Average reward for the grpo optimized transformer: {np.mean(np.array(post_reward))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_context = torch.tensor([naive_tokenizer.encode('\\n')], dtype=torch.long).to(device)\n",
    "grpo_generation, _ =  grpo_trans('generate',context=base_context, nb_tokens=1024)\n",
    "grpo_reward = full_reward(naive_tokenizer.decode(grpo_generation[0].tolist()))\n",
    "trans_generation, _ = transformer('generate', context=base_context, nb_tokens=1024)\n",
    "trans_reward = full_reward(naive_tokenizer.decode(trans_generation[0].tolist()))\n",
    "\n",
    "print(f'Rewards : GRPO {grpo_reward}, Base Transformer {trans_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Base transformer ({trans_reward} caps for 1024 tokens):')\n",
    "print(naive_tokenizer.decode(trans_generation[0].tolist()))\n",
    "\n",
    "print('----------------------------------------------------------------')\n",
    "print('----------------------------------------------------------------')\n",
    "print(f'GRPO updated transformer: ({grpo_reward} caps for 1024 tokens):')\n",
    "print(naive_tokenizer.decode(grpo_generation[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), 'base_transformer.pth')\n",
    "torch.save(grpo_trans.state_dict(), 'grpo_transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tload = DecoderTrans(transformer_params)\n",
    "tload.to(device)\n",
    "tload.load_state_dict(torch.load('base_transformer.pth'))\n",
    "\n",
    "grpo_trans = DecoderTrans(transformer_params)\n",
    "grpo_trans.to(device)\n",
    "grpo_trans.load_state_dict(torch.load('grpo_transformer.pth'))\n",
    "grpo_trans.train()\n",
    "\n",
    "print('Loaded the saved transformers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grpo_trans\n",
    "base_context = torch.tensor([naive_tokenizer.encode('\\n') for _ in range(1)], dtype=torch.long).to(device)\n",
    "generated_tokens, generated_logprobs =  model('generate',context=base_context, nb_tokens=1024)\n",
    "print(naive_tokenizer.decode(generated_tokens[0].cpu().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myInfEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
